{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Algorithme de Co-Clustering : Bregman Block Average (BBAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importons les packages nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Prétraitement de la base de données 20 Newsgroup\n",
    "\n",
    "#### Chargeons les données \n",
    "\n",
    "La documentation de Scikit-learn$^{[11]}$ détaille comment charger la base de données 20 Newsgroup. Celle-ci est une liste de plus de 18 000 posts de média, chacun associé à un label (parmi 20) spécifiant son thème. Chargeons les données et imprimons la dénomination des 20 catégories possibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Liste des catégories / thèmes des posts : \n",
      "\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "news = datasets.fetch_20newsgroups()\n",
    "print(\"\\nListe des catégories / thèmes des posts : \\n\")\n",
    "print(list(news.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restriction à une sous base de données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous souhaitons travailler avec moins de 18 000 posts et de 20 classes afin d'avoir des objets plus manipulables en termes de temps de calcul. Nous créons ainsi une sous base de données en choisissant de ne charger les données que pour deux catégories : \"espace\" et \"religion chrétienne\" (dans l'espoir que leur champs lexicaux soient éloignés). Nous profitons également de cette étape pour élimiter les en-têtes, les bas de page et les citations associés aux posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sci.space', 'soc.religion.christian']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = ['sci.space','soc.religion.christian']\n",
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'),categories=cats)\n",
    "list(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après restriction de nos données à ces deux catégories, nous n'avons plus que **1192 posts et 2 classes** (codées par le label 0 ou 1). On constate également que nos données ne sont pas ordonnées par label (comme c'était le cas pour la base des iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension du vecteur de labels :  (1192,)\n",
      "Exemple des dix premiers labels : [1 0 0 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension du vecteur de labels : \", newsgroups_train.target.shape) # target contient les labels des posts \n",
    "print(\"Exemple des dix premiers labels :\", newsgroups_train.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par ailleurs, la commande ci-dessous montre qu'il y a 593 posts de label=0 ('space') et 599 posts de label=1 ('religion'). Notre échantillon est donc équitablement réparti entre nos deux catégories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de posts associés au label=0 : 593\n",
      "Nombres de posts associés au label=1 : 599\n"
     ]
    }
   ],
   "source": [
    "print('Nombres de posts associés au label=0 :', sum(newsgroups_train.target==0))\n",
    "print('Nombres de posts associés au label=1 :', sum(newsgroups_train.target==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons un post associé au label=0 ('space') :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post de label :  0\n",
      "You have missed something.  There is a big difference between being in\n",
      "the SAME PLANE and in exactly the same state (positions and velocities\n",
      "equal).  IN addition to this, there has always been redundancies proposed.\n",
      "\n",
      "Bob\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "print(\"Post de label : \",newsgroups_train.target[1])\n",
    "print(newsgroups_train.data[1]) # data est une liste contenant les posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons un post associé au label=1 ('religion') :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post de label :  1\n",
      "I think it was Lewis who said that in a wedding, it's the principals  \n",
      "that marry each other; the church and the state are present merely as  \n",
      "witnesses.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Rob Steele                 In coming to understand anything \n",
      "MIT Lincoln Laboratory    we are rejecting the facts as they\n",
      "244 Wood St., M-203       are for us in favour of the facts\n",
      "Lexington, MA  02173      as they are.    \n",
      "617/981-2575                              C.S. Lewis\n"
     ]
    }
   ],
   "source": [
    "print(\"Post de label : \",newsgroups_train.target[0])\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le fait d'avoir retiré les bas de page et les citations des données n'a pas permis de parfaitement nettoyer ces dernières puisque l'exemple ci-dessus contient toujours ce qui semble être une signature contenant une citation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion des données en une matrice de décompte des \"tokens\"\n",
    "\n",
    "On transforme notre vecteur de 1192 posts en une matrice (sparse) de 1192 lignes (un post par ligne) et de 20419 colonnes (un mot par colonne). Ainsi, chaque post (il y a environ 20-100 mots par post) est représenté par un vecteur ligne indiquant le nombre d'occurrence des mots qui apparaissent dans le post sur la base d'un dictionnaire de taille 20419 (automatiquement généré par le module CountVectorizer de Scikit-learn). Nous avons volontairement choisi de ne pas normaliser les lignes de la matrice afin qu'elle soit bien une matrice de co-occurrence Post / Mot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la matrice générée :  (1192, 20419)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(newsgroups_train.data)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(\"Dimensions de la matrice générée : \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice générée est d'un type particulier, réservé aux matrices sparse : <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(\"La matrice générée est d'un type particulier, réservé aux matrices sparse :\", type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrait des \"stop words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raffinons ce prétraitement en retirant les \"stop words\", soit les mots récurrents de l'anglais qui sont connus pour être peu porteurs de sens en Natural language processing (comme \"$and$\", \"$the$\", \"$it$\", etc.). Pour ce faire, Scikit-learn permet d'utiliser une liste de notre choix qu'on passera en argument de la fonction CountVectorizer (qui transforme le texte en matrice). Après quelques recherches en ligne, nous avons sélectionné une liste de 851 stopwords$^{[12]}$ que nous avons reproduite ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "countwordslist=['able','about','above','abroad','according','accordingly','across','actually','adj','after','afterwards','again','against','ago','ahead','ain','t','all','allow','allows','almost','alone','along','alongside','already','also','although','always','am','amid','amidst','among','amongst','an','and','another','any','anybody','anyhow','anyone','anything','anyway','anyways','anywhere','apart','appear','appreciate','appropriate','are','aren','t','around','as','a','s','aside','ask','asking','associated','at','available','away','awfully','back','backward','backwards','be','became','because','become','becomes','becoming','been','before','beforehand','begin','behind','being','believe','below','beside','besides','best','better','between','beyond','both','brief','but','by','came','can','cannot','cant','can','t','caption','cause','causes','certain','certainly','changes','clearly','c','mon','co','co.','com','come','comes','concerning','consequently','consider','considering','contain','containing','contains','corresponding','could','couldn','t','course','c','s','currently','dare','daren','t','definitely','described','despite','did','didn','t','different','directly','do','does','doesn','t','doing','done','don','t','down','downwards','during','each','edu','eg','eight','eighty','either','else','elsewhere','end','ending','enough','entirely','especially','et','etc','even','ever','evermore','every','everybody','everyone','everything','everywhere','ex','exactly','example','except','fairly','far','farther','few','fewer','fifth','first','five','followed','following','follows','for','forever','former','formerly','forth','forward','found','four','from','further','furthermore','get','gets','getting','given','gives','go','goes','going','gone','got','gotten','greetings','had','hadn','t','half','happens','hardly','has','hasn','t','have','haven','t','having','he','he','d','he','ll','hello','help','hence','her','here','hereafter','hereby','herein','here','s','hereupon','hers','herself','he','s','hi','him','himself','his','hither','hopefully','how','howbeit','however','hundred','i','d','ie','if','ignored','i','ll','i','m','immediate','in','inasmuch','inc','inc.','indeed','indicate','indicated','indicates','inner','inside','insofar','instead','into','inward','is','isn','t','it','it','d','it','ll','its','it','s','itself','i','ve','just','k','keep','keeps','kept','know','known','knows','last','lately','later','latter','latterly','least','less','lest','let','let','s','like','liked','likely','likewise','little','look','looking','looks','low','lower','ltd','made','mainly','make','makes','many','may','maybe','mayn','t','me','mean','meantime','meanwhile','merely','might','mightn','t','mine','minus','miss','more','moreover','most','mostly','mr','mrs','much','must','mustn','t','my','myself','name','namely','nd','near','nearly','necessary','need','needn','t','needs','neither','never','neverf','neverless','nevertheless','new','next','nine','ninety','no','nobody','non','none','nonetheless','noone','no-one','nor','normally','not','nothing','notwithstanding','novel','now','nowhere','obviously','of','off','often','oh','ok','okay','old','on','once','one','ones','one','s','only','onto','opposite','or','other','others','otherwise','ought','oughtn','t','our','ours','ourselves','out','outside','over','overall','own','particular','particularly','past','per','perhaps','placed','please','plus','possible','presumably','probably','provided','provides','que','quite','qv','rather','rd','re','really','reasonably','recent','recently','regarding','regardless','regards','relatively','respectively','right','round','said','same','saw','say','saying','says','second','secondly','see','seeing','seem','seemed','seeming','seems','seen','self','selves','sensible','sent','serious','seriously','seven','several','shall','shan','t','she','she','d','she','ll','she','s','should','shouldn','t','since','six','so','some','somebody','someday','somehow','someone','something','sometime','sometimes','somewhat','somewhere','soon','sorry','specified','specify','specifying','still','sub','such','sup','sure','take','taken','taking','tell','tends','th','than','thank','thanks','thanx','that','that','ll','thats','that','s','that','ve','the','their','theirs','them','themselves','then','thence','there','thereafter','thereby','there','d','therefore','therein','there','ll','there','re','theres','there','s','thereupon','there','ve','these','they','they','d','they','ll','they','re','they','ve','thing','things','think','third','thirty','this','thorough','thoroughly','those','though','three','through','throughout','thru','thus','till','to','together','too','took','toward','towards','tried','tries','truly','try','trying','t','s','twice','two','un','under','underneath','undoing','unfortunately','unless','unlike','unlikely','until','unto','up','upon','upwards','us','use','used','useful','uses','using','usually','v','value','various','versus','very','via','viz','vs','want','wants','was','wasn','t','way','we','we','d','welcome','well','we','ll','went','were','we','re','weren','t','we','ve','what','whatever','what','ll','what','s','what','ve','when','whence','whenever','where','whereafter','whereas','whereby','wherein','where','s','whereupon','wherever','whether','which','whichever','while','whilst','whither','who','who','d','whoever','whole','who','ll','whom','whomever','who','s','whose','why','will','willing','wish','with','within','without','wonder','won','t','would','wouldn','t','yes','yet','you','you','d','you','ll','your','you','re','yours','yourself','yourselves','you','ve','zero','a','how','s','i','when','s','why','s','b','c','d','e','f','g','h','j','l','m','n','o','p','q','r','s','t','u','uucp','w','x','y','z','I','www','amount','bill','bottom','call','computer','con','couldnt','cry','de','describe','detail','due','eleven','empty','fifteen','fifty','fill','find','fire','forty','front','full','give','hasnt','herse','himse','interest','itse”','mill','move','myse”','part','put','show','side','sincere','sixty','system','ten','thick','thin','top','twelve','twenty','abst','accordance','act','added','adopted','affected','affecting','affects','ah','announce','anymore','apparently','approximately','aren','arent','arise','auth','beginning','beginnings','begins','biol','briefly','ca','date','ed','effect','et-al','ff','fix','gave','giving','heres','hes','hid','home','id','im','immediately','importance','important','index','information','invention','itd','keys','kg','km','largely','lets','line','','ll','means','mg','million','ml','mug','na','nay','necessarily','nos','noted','obtain','obtained','omitted','ord','owing','page','pages','poorly','possibly','potentially','pp','predominantly','present','previously','primarily','promptly','proud','quickly','ran','readily','ref','refs','related','research','resulted','resulting','results','run','sec','section','shed','shes','showed','shown','showns','shows','significant','significantly','similar','similarly','slightly','somethan','specifically','state','states','stop','strongly','substantially','successfully','sufficiently','suggest','thered','thereof','therere','thereto','theyd','theyre','thou','thoughh','thousand','throug','til','tip','ts','ups','usefully','usefulness','','ve','vol','vols','wed','whats','wheres','whim','whod','whos','widely','words','world','youd','youre', '15', '11', '19', '93', '34', '55', '14', '03', '01', '02', '39', '31', '18', '17', '60', '10', '30', '21', '24', '44', '33', '20', '04', '100', '35', '40', '13', '000', '12', '16', '28', '22', '23', '25', '41', '29', '38', '27', '32', '00', '08', '26', '36', '50', '05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la matrice après retrait des stop words :  (1192, 19686)\n"
     ]
    }
   ],
   "source": [
    "cv_stop = CountVectorizer(stop_words=countwordslist)\n",
    "X_train=cv_stop.fit_transform(newsgroups_train.data)\n",
    "print(\"Dimensions de la matrice après retrait des stop words : \", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que cette étape d'élimination des \"stopwords\" a retiré 733 mots de notre corpus (et donc 733 colonnes à notre matrice). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de l'information mutuelle dans l'étape de sélection des colonnes\n",
    "\n",
    "L'un des problèmes majeurs de notre base de données est qu'elle est, au-delà de sa sparsité, en très grande dimension puisque son nombre de colonnes est 16 fois plus élevé que son nombre de lignes. Cela pose notamment des problèmes en termes de temps de calcul. Nous souhaitons donc effectuer une étape supplémentaire de sélection des colonnes de la matrice. \n",
    "\n",
    "Afin d'adopter une approche valide, nous avons cherché des détails concernant le prétraitement réalisé par les auteurs de l'article dédié à l'algorithme BBAC$^{[1]}$. Bien que ceux-ci ne soient pas détaillés dans l'article, ni disponibles en ligne, un autre article$^{[4]}$ référencé par les références majeures de l'article BBAC nous a donné des détails concernant le prétraitement utilisé par ses auteurs sur la base 20 Newsgroup :\n",
    "\n",
    "\"*Our pre-processing [...] included a standard feature selection mechanism, where for each dataset we selected the 2000 words with the highest contribution to the **mutual information between the words and the documents**. More formally stated, for each dataset, we sorted all words by\n",
    "$I(y)≡p(y)\\sum_{x \\in X}p(x|y)\\log \\left( \\frac{p(x|y)}{p(x)} \\right)$ and selected the top 2000.*\"$^{[4]}$\n",
    "\n",
    "Dans notre cas, $x$ est un post ($x \\in X$) et $y$ un mot ($y \\in Y$). \n",
    "On notera $N(x,y)$ le nombre d'occurrences du mot $y$ dans le post $x$ (valeur stockée dans notre matrice au niveau de la ligne $x$ et de la colonne $y$), $N(x)$ le nombre de mots total (avec répliques) dans le post $x$ (somme des valeurs stockées à la ligne $x$ de la matrice), $N(y)$ le nombre d'occurrences du mot $y$ dans le corpus (somme des valeurs stockées dans la colonne $y$ de la matrice) et $N$ le nombre total (avec répliques) de mots du corpus (somme des valeurs de la matrice).\n",
    "\n",
    "Avec ces notations, on peut réécrire :\n",
    "* la fréquence du mot $y$ dans le corpus : $p(y) = \\sum_{x\\in X} \\frac{N(x,y)}{N}$\n",
    "* la fréquence du post $x$ dans le corpus : $p(x) = \\sum_{y\\in Y} \\frac{N(x,y)}{N}$\n",
    "* la fréquence jointe du post $x$ et du mot $y$ dans le corpus : $p(x,y) =\\frac{N(x,y)}{N}$\n",
    "\n",
    "Ainsi on peut réécrire la formule de l'information $I(y)$, donnée dans la citation ci-dessus, comme suit : \n",
    "$I(y)=\\sum_{x \\in X}p(x,y)\\log \\left( \\frac{p(x,y)}{p(y)p(x)} \\right) =\\sum_{x \\in X}\\frac{N(x,y)}{N}\\log \\left(\\frac{N(x,y)N}{N(x)N(y)} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Ne pas exécuter cette section du code\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons l'information mutuelle des mots du corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n= 1192,19686 # nombre de lignes m et de colonnes n de la matrice :\n",
    "N=np.sum(X_train.data)\n",
    "\n",
    "# Stockons toutes les valeurs N(x) et N(y) sous forme de vecteurs\n",
    "Nx_vec=np.sum(X_train,axis=1)\n",
    "Ny_vec=np.sum(X_train,axis=0)\n",
    "Ny_vec=np.reshape(Ny_vec,(n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vecteur stockant les valeurs I(y):\n",
    "I_y_vec=np.zeros(n)\n",
    "\n",
    "# Timing de la fonction :\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for y in range(n): # boucle dans les colonnes (mots)\n",
    "    I_y=0 \n",
    "    Ny=Ny_vec[y]\n",
    "    for x in range(m): # boucle dans les lignes (posts) \n",
    "        Nxy=X_train[x,y]\n",
    "        if Nxy!=0:\n",
    "            I_y=I_y+(Nxy/N)*np.log((Nxy*N)/(Nx_vec[x]*Ny))\n",
    "    I_y_vec[y]=I_y\n",
    "\n",
    "toc = time.perf_counter()\n",
    "# print(f\"Le calcul des valeurs I(y) a pris {toc - tic:0.4f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque cette opération de calcul de l'information mutuelle des mots prend en moyenne plus de 50 min de calcul, nous allons extraire le résultat obtenu dans un fichier pour ne pas avoir à ré-exécuter cette section du code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsorted_Iy_list.txt\", \"w\") as external_file: \n",
    "    for I_y in I_y_vec:\n",
    "        add_text = I_y\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trions les indices du vecteur **I_y_vec** (par ordre décroissant) afin de pouvoir sélectionner les mots (donc les colonnes) ayant la plus grande information mutuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_y_sorted_index=np.argsort(I_y_vec) # fonction de tri par ordre croissant\n",
    "I_y_sorted_index=np.flip(I_y_sorted_index) # inversion de l'array pour avoir l'ordre décroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On extrait de même dans un fichier la liste des indices des mots triés par information mutuelle décroissante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Iy_sorted_index.txt\", \"w\") as external_file:\n",
    "    for index in I_y_sorted_index:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Fin de la section à ne pas exécuter\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reproduit ci-dessous les indices extraits des 2000 mots ayant la plus haute information mutuelle : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2000_index=np.array([8475,16613,10267,4218,13344,12223,7508,3180,4184,10742,4192,4195,8977,18263,17885,14517,10753,8501,7577,10937,6552,5514,11187,12001,18252,16312,16234,16555,16734,12878,15730,14735,11320,4194,19603,19606,9364,13267,11137,13732,11108,15886,5538,8936,11438,19471,3390,19464,14714,3911,14127,13881,9152,11092,13794,7187,16913,2797,14698,5548,7470,14184,15001,3352,9257,16520,11855,3100,2260,15834,7109,8663,16615,11122,11450,19532,10304,13553,11412,5149,15833,7289,7286,13428,13848,8919,18571,17814,15028,15263,17594,11418,7805,12378,13630,3101,3721,3981,9048,2411,4266,384,16525,5560,3393,11486,5569,8612,11769,4538,11036,17272,11966,11272,15622,18479,2986,14525,17761,2613,14125,15448,15998,16847,15304,7627,16445,8780,10698,2532,1586,19479,7295,5932,6757,3195,14204,12250,4513,7330,10714,6564,3208,5544,8521,17939,15731,7875,18958,15005,11545,10950,2470,11007,14704,16575,17702,10531,3870,8820,17156,11911,16302,10709,10275,5510,12238,17338,8271,8542,14138,4305,3249,9449,17894,12045,7552,9944,10277,18576,12930,3265,4150,19151,2459,18857,8528,1625,11414,9179,5999,6387,8037,11399,19247,16592,8098,11652,15753,11858,8384,17508,19494,18483,5007,6544,12945,10162,14376,6164,17460,12528,9945,16005,5400,17317,4056,18805,13645,19183,11401,1930,11391,2899,6406,10743,11755,16339,14721,8702,16072,16665,16834,9096,17510,1759,17623,15605,12888,5856,15452,13361,6874,19530,17135,8137,1727,10571,10207,14052,9976,5109,13290,11042,5179,12879,17952,15003,4220,15678,13914,19477,17914,14130,13857,15808,16909,17965,7456,10074,2644,19148,12592,17792,16861,19447,10820,10586,8666,18881,16590,3608,14283,9734,6329,11618,2566,6299,1961,10406,13642,10875,16505,4702,17133,9369,11293,5713,9134,2698,14969,14841,14445,3775,16897,15225,12833,14576,13830,12343,1903,13860,10294,12910,11470,16736,2653,14238,5529,2140,11041,17573,3182,6887,8918,17673,4516,17326,13432,17584,2410,16889,16384,1829,11916,9589,3977,3614,8067,15232,8157,7346,15152,2678,2740,2053,1734,13467,13852,12738,9592,13278,18259,8808,7717,6012,17037,16198,16594,17704,19344,14328,8104,11033,14193,16325,9361,15489,2995,14126,16353,8806,10746,18349,19320,9967,4073,10370,8968,2255,9709,6124,10759,11300,13916,4170,7696,2984,16668,4925,12867,12881,1952,14869,7946,4022,5258,17274,11093,16682,19536,4348,12663,10329,5154,4000,8514,2342,2999,7312,14871,14107,4284,12010,1855,1631,8278,15745,11120,14115,15212,4886,6160,9971,15724,12429,14663,14723,8069,5699,19449,7902,6640,15043,7539,10541,17130,11179,7192,15825,11924,15285,2949,2373,11059,4585,3246,15752,3410,12665,14546,9597,15585,17105,14599,9492,3528,2519,16064,13245,2993,16010,15662,14640,2676,10349,8285,11639,13090,16846,15887,7633,12577,17574,10680,16095,10372,12296,2393,4575,19197,15836,16293,15485,17552,18077,4560,8896,2638,17575,7516,17511,11488,15652,16672,2137,6885,6890,7992,13919,13215,4258,8607,19212,5295,7715,2203,8127,8720,2518,17639,12248,15153,3667,8660,9161,18302,16061,9024,3828,11703,10870,10584,2650,9862,5562,18062,3104,15451,3447,14739,7051,5928,1963,10824,10540,17369,2329,3111,16349,19528,6960,5420,4502,11611,1651,447,17680,9138,7525,4722,16717,4002,14373,15102,3559,12638,15254,1588,5603,3596,416,7703,8622,10685,8139,11487,10164,7686,12681,5081,4107,12815,17723,14026,11549,18063,12869,9239,2616,15770,11994,13225,10921,14565,16895,8183,11627,13068,10727,5078,8213,17327,4145,3996,8034,13896,12382,1714,9217,2264,13638,2926,1748,19386,5698,4270,7364,4868,14020,6661,17961,18915,17074,3316,6021,4166,7316,2452,16954,7086,10889,8196,9452,18913,14558,15148,2267,16582,8469,4953,1907,15900,7629,17991,11656,18963,14318,4813,5243,16706,12173,9445,6013,5158,11002,16263,9097,884,7597,12413,5238,12834,1728,6723,2170,2617,18014,14740,15476,8925,10783,3913,13274,12243,17754,2444,10354,18311,19627,9911,13558,13138,7874,10805,4268,6070,16878,5735,15112,14252,1723,7181,17113,5679,4556,17556,2359,14736,1587,4524,6000,14586,13381,5215,2471,9049,18016,4057,15106,7642,14427,5743,11628,6647,19523,14766,9298,5263,7883,2460,11186,11936,13178,7467,15403,1559,14616,13224,5380,16008,5666,19500,1715,12584,16924,11123,17501,4630,16606,19599,9091,7179,6255,14953,3294,15499,15903,10730,14322,10798,11617,11078,18326,15889,16131,7758,13435,11956,18796,9268,1628,12403,10307,6161,9218,4113,16048,11247,18868,16898,14169,6087,11791,14767,12627,18126,4632,11037,15226,4583,13596,6410,11395,14759,12843,5480,16715,16879,5894,9591,12731,9054,7934,6542,8833,7225,8118,12884,14447,19492,2726,6081,8480,16537,1272,2474,14795,3676,8816,7352,383,7177,7113,7335,4436,10799,4530,19629,16099,17569,12050,13866,2831,14022,10457,2041,13894,9685,5602,11473,1739,14008,6819,8130,14701,11352,3792,13761,17979,6392,18719,10840,6562,1243,3857,19285,11117,10856,14076,9590,18560,6331,13640,6465,8515,15155,7140,2881,3920,3611,10660,18804,5086,6662,14251,4269,11589,16110,3204,7433,7846,14684,12596,5780,12826,19050,12026,12633,10800,9668,4532,3563,17900,2610,3793,19282,15809,9410,16075,17652,5705,11087,11910,8948,19138,18221,13587,8390,15210,19006,11098,2185,6653,6458,11279,4703,5378,10492,4567,5523,10299,4473,6882,7806,1724,12153,5861,1613,18488,8483,18563,4324,10710,15913,13230,7294,3871,15956,13518,4405,5174,11379,10932,11591,15944,10744,18267,1242,10975,633,18914,14790,8388,1598,12075,13715,2734,11324,19008,12709,14462,14211,14117,3583,4981,8300,14384,16910,14273,8154,11244,16025,10154,7329,16585,8291,14814,9900,10316,4552,7502,17337,14260,6074,14073,10411,2694,16928,15478,5239,13739,11850,7495,10788,9541,14137,9345,981,10656,10513,18538,19216,3082,18367,5093,10608,13631,5791,8357,12902,8254,11202,11810,11610,12112,17126,17509,16842,16880,4352,17440,3108,16359,5905,6332,15387,11748,15492,16908,14154,13556,5499,15299,7568,13086,10872,7162,3767,10263,6165,18878,15581,10707,4084,17857,8087,475,4709,10408,8728,8180,17230,7139,5486,19453,10815,8947,1810,14257,16264,2376,3253,1755,18245,3720,17727,15108,2928,10699,17985,19258,19162,17716,15604,2533,8613,17847,17722,5031,2341,10765,14813,2438,14160,5651,1756,1789,6849,1156,4457,18786,5195,2387,3832,18554,2102,4536,13846,1669,4874,11922,6572,379,9880,7218,3135,15434,7095,12963,12883,18329,2796,13653,5115,6939,12036,10985,15011,10517,9724,12649,12210,17993,17526,14866,4950,6308,18716,8443,12668,6563,9834,19334,5766,19491,14246,15237,17793,2135,19401,17759,15839,7062,10098,11866,2204,16838,12692,17593,15014,4260,18055,4761,5276,377,10519,2836,11727,10522,2649,19011,6799,11374,5048,1811,10140,8993,9139,17820,13440,4637,8645,3107,5008,7490,2499,15647,7422,14547,7252,14988,5845,7850,13180,4570,2756,11520,10877,19077,3038,1087,13368,10466,17207,12612,19250,17631,10819,7332,2218,9160,13920,5614,5926,16350,18682,8050,1718,14092,13096,11216,18375,17632,12613,2374,15483,2212,11661,4006,11653,13858,11276,7535,2647,14161,2573,18717,14916,15340,11729,5135,17296,17983,19451,1945,3416,13648,15101,2525,13882,2851,7349,16751,2108,3248,9137,7478,16596,6198,7350,4787,11114,1620,16728,17610,4747,2357,16058,4963,1501,17545,13836,13441,11416,1891,14219,13223,6571,14710,2891,767,4555,9531,8172,18708,4523,14311,10273,15383,19187,10474,15659,16031,8056,7592,2568,18102,17269,3344,3562,10682,2372,7578,11145,10398,10337,15633,12543,13565,7679,12824,11640,2115,16900,10312,19371,5162,2262,14985,4983,19261,11425,12237,13834,7173,8432,8548,9168,5867,5373,10217,7847,14174,19504,5070,14570,2232,8135,14692,6142,14153,7206,5645,14331,4349,11992,12857,12825,11282,5940,4892,14667,7066,7112,11138,12734,13364,7343,12810,12385,12351,9403,7774,16246,17572,8907,18969,17839,10343,4475,1577,14309,2601,5197,13485,18305,17121,1691,14081,5349,3400,17768,9622,5785,12814,16805,13672,18304,13091,13874,7473,10377,2631,1626,7020,11687,16219,13367,3877,12778,17756,11464,13198,3092,13495,4474,11750,10138,6720,12931,15426,13241,7466,3303,2729,7007,17186,6351,18858,2275,16056,4285,8537,15291,12207,13354,11993,14968,1656,9032,13811,7423,11079,5459,16276,17547,11853,3373,3984,7709,6184,7623,6605,13733,15820,10976,16669,15759,11796,11553,17919,10774,16044,3025,12661,2065,10308,17346,17315,8039,11808,11456,10731,13801,4990,13650,8709,3659,17969,11744,833,1761,1831,14324,13970,3239,5485,4723,13279,17128,13775,17322,18692,6278,8576,11009,17141,7509,10973,12841,15156,13960,15693,2618,13791,1836,11168,3463,9975,12260,1569,16856,10371,18378,2020,16716,4341,14220,8343,9743,12509,6527,13310,7800,3965,15664,16151,3971,9454,14247,16127,6336,3339,9979,19123,14091,13924,6383,2465,15229,1069,17987,19529,5893,12644,3968,7706,15606,13980,3891,19193,2720,8601,14875,18181,3293,15669,7621,14618,5497,3986,13202,10944,15219,6514,17276,17174,8477,9136,17129,13509,12732,5236,11350,4708,10364,15395,12922,12802,7288,19175,5891,17236,7340,11876,6166,10157,15977,14270,15230,12147,11494,2375,5282,9216,16335,11294,417,6023,2167,4705,2768,16556,14700,5102,5954,10062,7958,4232,1681,12305,2086,7959,2790,16456,8141,18084,7491,18053,10295,846,14437,11191,19429,7628,10786,16703,13853,8299,10219,11201,5347,9264,6837,17528,17568,16319,3069,13226,8928,13716,7891,13016,14239,6805,7616,17614,8960,5675,2614,18020,8382,6955,9767,6923,11683,15744,14876,12912,16770,2675,10647,15398,13799,2780,10391,12079,7249,18567,13159,7700,6029,15801,5181,16724,3498,11304,15406,15954,15661,4041,17892,19616,8243,8846,3565,13285,14068,1780,15970,9708,12823,5977,11863,7994,6459,13137,10909,7937,11813,3548,13097,17606,2700,15592,7248,15396,16497,9918,6657,3050,18944,11573,9375,14066,2046,12530,17636,5906,17714,18871,13922,8981,10320,1889,7752,19203,4167,9003,9877,18128,19249,11555,820,16069,11162,6304,9434,11022,7877,8979,8810,18691,4810,5674,3440,9402,19328,15404,8202,9991,8113,4261,14598,2205,5173,3851,11018,7122,4778,12342,14000,8938,7947,6560,14540,4976,17612,2769,15915,16149,16257,12765,8192,11370,8708,11068,10268,13338,19512,10787,15151,8914,19466,943,7744,13566,13898,4451,7520,11011,18591,3399,14140,8729,13568,16345,16584,7638,6443,19427,7915,13652,8489,12929,9576,17737,7398,1297,11664,12074,6664,14423,10186,14012,3833,3116,3994,19239,14010,8675,11799,10073,16238,3652,15835,15967,16789,2077,11556,11546,3627,10936,10670,2935,48,12895,6674,4106,8604,6324,14065,2489,11484,3937,4172,19474,9467,10412,16612,16620,18749,18173,3311,19510,14205,8488,4834,13756,12761,16701,7017,19248,8121,7611,12642,18835,4655,5850,74,5815,3723,15334,4865,5567,12037,3219,13321,8054,14522,10285,4317,10713,19034,6632,6492,6078,12987,1707,5448,17281,14387,2824,9116,18060,19621,7870,14454,7722,14906,7447,13546,17283,10133,5823,14141,13662,7034,12512,16588,4661,17544,12115,8176,2630,19673,10711,2585,5196,6391,1999,6670,15288,9558,5389,9844,5198,5900,5792,12598,10130,6611,11680,19196,7381,16903,18146,3349,15929,15286,3014,9624,6373,18178,4906,6261,18036,4590,11983,1647,9112,14210,18740,6450,3644,2801,18709,10026,8614,2509,11353,16106,2192,7052,9163,9965,13557,14771,16038,9670,1878,8790,7950,1742,19602,1629,7336,7725,11008,10717,16307,14438,9413,11142,9223,11017,11624,4361,12742,19133,373,5248,14975,13073,8227,5573,15361,4143,19125,15446,17904,4859,3725,14992,5105,3879,15294,11773,18900,4831,8929,12515,13156,5584,15593,7630,8866,4059,8061,17279,17187,15115,9340,17042,16722,10416,8616,6565,2730,14614,18778,3791,3421,13268,10808,17804,13395,2982,17228])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi sélectionner les colonnes de notre matrice sur la base de l'information mutuelle des mots (colonnes) de celle-ci. L'article a sélectionné 2000 mots en travaillant avec 3500 posts (lignes de la matrice). Notre matrice ayant trois fois moins de lignes, nous sélectionnerons non pas 2000 colonnes mais 700 seulement ce qui est relativement proportionnel et permettra d'avoir des temps de calcul acceptables pour notre application. \n",
    "Effectuons cette sélection des colonnes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "top700_index=top2000_index[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle dimension de la matrice :  (1192, 700)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train[:,top700_index] # slice selon les colonnes d'intérêt\n",
    "print(\"Nouvelle dimension de la matrice : \", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Algorithme BBAC \n",
    "\n",
    "On notera :\n",
    "* $m$ : le nombre de lignes de la matrice\n",
    "* $n$ : son nombre de colonnes\n",
    "* $k$ : son nombre de clusters de lignes\n",
    "* $l$ : son nombre de clusters de colonnes\n",
    "* $z_{uv}$ : entrée de la matrice, stockée à la ligne $u$, colonne $v$ \n",
    "\n",
    "Bien que l'article BBAC$^{[1]}$ invite à choisir une mesure de probabilité $w$ sur les éléments de la matrice, nous ferons tout au long de ce projet et par souci de simplicité, le choix de la mesure de probabilité uniforme sur les éléments de la matrice : \n",
    "\n",
    "$\\forall (i,j) \\in \\{0,...,m-1\\} \\times \\{0,...,n-1\\}, w_{i,j}=\\frac{1}{mn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liens et différences avec K-means :\n",
    "\n",
    "L'article BBAC$^{[1]}$ détaille p. 1934 les étapes de l'algorithme. Similairement à K-means, BBAC a pour objectif de minimiser une fonction objectif de manière itérative. Les différences principales de cet algorithme avec K-means sont que :\n",
    "* l'algorithme aura une étape de clustering sur les lignes (on notera la fonction d'assignation $\\rho$) puis sur les colonnes (avec fonction d'assignation notée $\\gamma$) alternativement lors de sa phase itérative\n",
    "* dans notre approximation de la matrice d'origine, il faudra préserver une statistique : la moyenne du bloc de co-clustering $(g,h)$, qu'on notera $\\mu_{gh}$ avec $g$ $\\in \\{0,...k-1\\}$ et $h$ $\\in \\{0,...,l-1\\}$\n",
    "* le calcul de dissimilarité entre une entrée de la matrice $z_{uv}$ et la moyenne d'un bloc de co-clustering $\\mu_{gh}$ pourra se faire à l'aide d'une divergence $d_{\\phi}(z_{uv},\\mu_{gh})$, symétrique ou non, au choix au sein des divergences de Bregman\n",
    "* la fonction objectif (qu'on notera $f$), donnée à la p. 1932 (équation (14)) de l'article$^{[1]}$ est définie comme suit, avec $u \\in \\{1,...,m\\}, v\\in\\{1,...,n\\}$ :\n",
    "\n",
    "$f(\\mu,\\rho,\\gamma)=\\mathbb{E}[d_{\\phi}(z_{uv},\\mu_{\\rho(u)\\gamma(v)})]=\\frac{1}{nm}\\sum_{u,v}d_{\\phi}(z_{uv},\\mu_{\\rho(u)\\gamma(v)})$\n",
    "\n",
    "\n",
    "#### Etapes de l'algorithme : \n",
    "\n",
    "* **Initialisation des fonctions d'assignation $\\rho$ et $\\gamma$**\n",
    "\n",
    "Cette initialisation pourra se faire de manière aléatoire (uniforme) ou informée (à l'aide d'un autre algorithme de clustering).\n",
    "\n",
    "* **Phase itérative au temps t (jusqu'à convergence)**\n",
    "\n",
    "> Etape A : mise à jour des moyennes des blocs de co-clustering $\\mu_{gh}$\n",
    "\n",
    "$$\\forall g,h \\in \\{0,...,k-1\\} \\times \\{0,...,l-1\\}, \\mu_{gh}^{(t)}=\\frac{\\sum_{u:\\rho(u)=g} \\sum_{v:\\gamma(v)=h} z_{uv}}{\\#\\{(u,v): \\rho(u)=g, \\gamma(v)=h\\}}$$\n",
    "\n",
    "> Etape B : mise à jour des clusters de ligne ($\\rho$)\n",
    " \n",
    "$$\\forall u \\in \\{0,...,m-1\\}, \\rho^{(t)}(u)=\\mathop{\\textrm{argmin}}_{g\\in\\{0,...,k-1\\}} \\frac{1}{nm} \\sum_{h=0}^{l-1} \\sum_{v:\\gamma(v)=h} d_{\\phi}(z_{uv},\\mu_{gh})$$\n",
    " \n",
    "> Etape C : mise à jour des clusters de colonne ($\\gamma$)\n",
    " \n",
    "$$\\forall v \\in \\{0,...,n-1\\}, \\gamma^{(t)}(v)=\\mathop{\\textrm{argmin}}_{h\\in\\{0,...,l-1\\}} \\frac{1}{nm} \\sum_{g=0}^{k-1} \\sum_{u:\\rho(u)=g} d_{\\phi}(z_{uv},\\mu_{gh})$$\n",
    "\n",
    "* **Critère d'arrêt**\n",
    "\n",
    "Ce critère met fin à la phase itérative dans deux cas de figure :\n",
    "\n",
    "- si le nombre d'itérations dépasse un nombre maximal fixé \n",
    "\n",
    "- si chaque nouvelle itération ne fait plus diminuer la fonction objectif de plus d'un certain seuil, noté $\\epsilon$, comme suit :\n",
    "$\\left| f(\\mu^{(t)},\\rho^{(t)},\\gamma^{(t)})-f(\\mu^{(t-1)},\\rho^{(t-1)},\\gamma^{(t-1)})\\right|< \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Données en entrée de l'algorithme**\n",
    "\n",
    "Nous allons appliquer l'algorithme de co-clustering BBAC (Bregman block-average co-clustering) à la matrice X_train qui a $m=1192$ lignes et $n=700$ colonnes. Ayant deux labels (\"religion\" et \"space\"), on choisit d'effectuer un co-clustering avec un nombre de clusters de ligne $k=2$ fixe - cela nous permettra ainsi de comparer les labels obtenus aux labels réels. En ce qui concerne le nombre de clusters de colonnes, ni notre base de données ni l'algorithme ne nous contraignent à un choix particulier. Notons qu'un aspect de la mise en oeuvre de l'algorithme sera dédié, comme le font les auteurs de l'article BBAC$^{[1]}$ p. 1957, à comparer les résultats pour un nombre de clusters de colonnes variable. Les auteurs ont choisi d'explorer les résultats pour $l$ variant de 5 à 50 (en travaillant avec une autre base de données, CLASSIC3). Dans un premier temps, afin de mettre en place l'algorithme et en nous inspirant de leur démarche, nous choisirons \"naïvement\" un nombre de clusters de colonnes $l=5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous reste à choisir comme donnée d'entrée la divergence $d_{\\phi}$. L'article BBAC $^{[1]}$ présente trois divergences d'intérêt à la p.1925, définies comme suit : \n",
    "\n",
    "* **Distance euclidienne au carré** : $\\text{pour }z_{1},z_{2} \\in \\mathbb{R},  d_{\\phi}(z_{1},z_{2})=(z_{1}-z_{2})^{2}$\n",
    "\n",
    "* **I-divergence** : $\\text{pour }z_{1},z_{2} \\in \\mathbb{R^{*}_{+}},  d_{\\phi}(z_{1},z_{2})=z_{1}\\log(\\frac{z_{1}}{z_{2}})-(z_{1}-z_{2})$\n",
    "\n",
    "* **Itakura-Saito Distance** : $\\text{pour }z_{1},z_{2} \\in \\mathbb{R^{*}_{+}},  d_{\\phi}(z_{1},z_{2})=\\frac{z_{1}}{z_{2}}-\\log(\\frac{z_{1}}{z_{2}})-1$\n",
    "\n",
    "On notera que ces divergences ne sont pas toutes symétriques et que dans notre cas d'étude, nous avons :\n",
    "* $z_{1}$ : une entrée de notre matrice posts / mots (avec $z_{1} \\in \\mathbb{N}$)\n",
    "* $z_{2}$ : la moyenne d'un bloc de co-clustering (avec $z_{2}\\geq0$)\n",
    "\n",
    "Notons également que parmi ces trois divergences, seule la distance euclidienne au carré est définie pour des données à valeurs dans $\\mathbb{R}$ quand les deux autres (la I-divergence et la Itakura-Saito Distance) ne sont définies que pour des valeurs strictement positives. Cela sera un point d'attention de notre étude étant donné que nos données (entrées de la matrice comme moyennes des blocs) sont positives ou nulles.\n",
    "\n",
    "Ainsi, nous ne mettrons pas en oeuvre la Itakura-Saito Distance, nous adapterons les formules de la I-divergence et nous utiliserons sans modification la distance euclidienne.\n",
    "\n",
    "* **Adaptation des formules de la I-divergence aux données de notre problème** \n",
    "\n",
    "Dans le cadre de la I-divergence, le cas où $z_{1}=0$ peut être traité en rappelant que $\\lim\\limits_{x \\rightarrow 0+} x \\log x =0$. Ainsi nous aurons recours à un prolongement par continuité permettant de travailler avec les formules suivantes : \n",
    "\n",
    "$\\text{pour }z_{2}>0 :$\n",
    "\n",
    "$d_{\\phi}(z_{1},z_{2})=z_{1}\\log(\\frac{z_{1}}{z_{2}})-(z_{1}-z_{2})\\text{ si }z_{1}>0\\text{  }(*)$\n",
    "\n",
    "$d_{\\phi}(z_{1},z_{2})=z_{2}\\text{ si }z_{1}=0 $\n",
    "\n",
    "Le sujet est plus délicat dans le cas où $z_{2}=0$ puisque celui-ci apparaît au dénominateur. Après consultation avec Mme Matias, nous avons convenu d'utiliser la convention de calcul suivante :\n",
    "\n",
    "Si $z_{2}=0$, nous poserons $z_{2}=\\epsilon$ afin de pouvoir utiliser la formule $(*)$ ci-dessus. Dans notre mise en oeuvre, nous utiliserons la valeur $\\epsilon=\\frac{1}{nm}$.\n",
    "\n",
    "Nous choisirons donc de mettre en oeuvre deux versions de l'algorithme BBAC : \n",
    "* une version utilisera comme divergence la **distance euclidienne au carré** (que nous noterons \"euclid\" dans le code)\n",
    "* une autre version utilisera la **I-divergence** (que nous noterons \"I-div\" dans le code) avec les adaptations mentionnées\n",
    "\n",
    "Cela correspond également au choix de divergence effectué par les auteurs de l'article BBAC $^{[1]}$ lors de leur application à l'analyse textuelle (p.1955-1957) bien qu'ils n'aient pas détaillé leur façon d'utiliser la I-divergence dans le cas de données nulles. Remarquons que le fait que les ensembles de définition des données soient restreints pour plusieurs divergences est une limite du \"meta algorithme\" proposé par l'article. En effet, en pratique, le choix de divergence sera contraint ou impliquera de nombreuses précautions comme nous venons de le voir pour la I-divergence. \n",
    "\n",
    "* **Remarque :**\n",
    "\n",
    "La divergence de Bregman considérée est censée ne pas avoir d'impact comme on le lit p.17 de l'article BBAC$^{[1]}$:\n",
    "\"*For block average co-clustering, the MBI solution is the same for all Bregman divergences (Theorem 1).*\" L'algorithme BBAC est donc censé converger vers une même solution théorique quelle que soit la divergence utilisée. Les auteurs ont pourtant souligné obtenir de meilleurs résultats pour la I-divergence plutôt que pour la distance euclidienne au carré. Afin de croiser nos résultats, nous essayerons aussi d'effectuer cette comparaison de convergence dans le cadre de notre application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Mise en oeuvre en Python\n",
    "\n",
    "#### Présentation de deux initialisations possibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,k,l=1192,700,2,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Initialisation aléatoire du co-clustering $(\\rho, \\gamma)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\rho: (0,...,m-1) \\rightarrow {\\{0,1\\}}^{m} $$\n",
    "\n",
    "$$ \\gamma : (0,...,n-1) \\rightarrow {\\{0,...,4\\}}^{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons montré plus haut que 593 posts sont de label=0 et 599 posts de label=1, ce qui est presque une répartition à 50%-50%. Nous allons donc initialiser aléatoirement les lignes comme étant de cluster ligne 0 ou 1 ; nous simulerons la distribution uniforme sur $\\{0,1\\}$ à l'aide de la fonction *randint* de Python.\n",
    "\n",
    "En ce qui concerne les champs lexicaux (donc les colonnes) nous n'avons aucune information intiale à injecter. On initialisera donc de même les colonnes de manière aléatoire à l'aide de la fonction randint de Python parmi les labels $\\{0,...,4\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation(m,n,k,l):\n",
    "    rho=np.random.randint(0, k, m)\n",
    "    gamma=np.random.randint(0, l, n)\n",
    "    return rho, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho,gamma=initialisation(m,n,k,l) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats de l'initialisation aléatoire:\n",
      "On a 589 lignes de cluster ligne 0 et 603 lignes de cluster ligne 1\n",
      "\n",
      "On a 142 colonnes de cluster colonne 0, \n",
      "On a 110 colonnes de cluster colonne 1, \n",
      "On a 133 colonnes de cluster colonne 2,\n",
      "On a 169 colonnes de cluster colonne 3,\n",
      "On a 146 colonnes de cluster colonne 4.\n"
     ]
    }
   ],
   "source": [
    "print(\"Résultats de l'initialisation aléatoire:\")\n",
    "print(f\"On a {sum(rho==0)} lignes de cluster ligne 0 et {sum(rho==1)} lignes de cluster ligne 1\")\n",
    "print(f\"\\nOn a {sum(gamma==0)} colonnes de cluster colonne 0, \\nOn a {sum(gamma==1)} colonnes de cluster colonne 1, \\nOn a {sum(gamma==2)} colonnes de cluster colonne 2,\\nOn a {sum(gamma==3)} colonnes de cluster colonne 3,\\nOn a {sum(gamma==4)} colonnes de cluster colonne 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b) Initialisation par K-means préalable :**\n",
    "\n",
    "Une autre initialisation possible repose sur l'utilisation du module K-means de Scikit-learn$^{[10]}$ afin d'effectuer un premier clustering sur les lignes puis sur les colonnes (après transposition de la matrice). Cette approche a pour but de partir d'un point initial plus informé qu'en ayant recours à une initialisation aléatoire uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kmeans(k,l,X_train):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++').fit(X_train)\n",
    "    rho=kmeans.labels_\n",
    "    kmeans = KMeans(n_clusters=l, init='k-means++').fit(np.transpose(X_train))\n",
    "    gamma=kmeans.labels_\n",
    "    return rho, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho,gamma=init_kmeans(k,l,X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats de l'initialisation via k-means:\n",
      "On a 14 lignes de cluster ligne 0 et 1178 lignes de cluster ligne 1\n",
      "\n",
      "On a 7 colonnes de cluster colonne 0, \n",
      "On a 669 colonnes de cluster colonne 1, \n",
      "On a 18 colonnes de cluster colonne 2,\n",
      "On a 5 colonnes de cluster colonne 3,\n",
      "On a 1 colonnes de cluster colonne 4.\n"
     ]
    }
   ],
   "source": [
    "print(\"Résultats de l'initialisation via k-means:\")\n",
    "print(f\"On a {sum(rho==0)} lignes de cluster ligne 0 et {sum(rho==1)} lignes de cluster ligne 1\")\n",
    "print(f\"\\nOn a {sum(gamma==0)} colonnes de cluster colonne 0, \\nOn a {sum(gamma==1)} colonnes de cluster colonne 1, \\nOn a {sum(gamma==2)} colonnes de cluster colonne 2,\\nOn a {sum(gamma==3)} colonnes de cluster colonne 3,\\nOn a {sum(gamma==4)} colonnes de cluster colonne 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate qu'un label semble aspirer la majeure partie des colonnes et de même au niveau des lignes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase itérative (étapes A,B et C) jusqu'à convergence :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Etape A : Calcul des $k \\times l$ \"MCC\" (moyennes des co-clusters)**\n",
    "\n",
    "On appelle MCC la matrice de taille $k \\times l$ qui contient à l'indice $(g,h)$ la moyenne du co-cluster $\\mu_{gh}$ (avec $g$ parcourant les clusters lignes $\\{0,...,k-1\\}$ et $h$ les clusters colonnes $\\{0,...,l-1\\}$).\n",
    "Définissons une fonction qui puisse utiliser comme divergence, la **distance euclidienne au carré** (via l'argument euclid=True) ou la **I-divergence** (via l'argument euclid=False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mcc(rho,gamma,X_train,k,l,euclid=True):\n",
    "    m,n=np.shape(X_train)[0],np.shape(X_train)[1]\n",
    "    MCC=np.zeros((k,l)) \n",
    "    for g in range(k): # boucle dans les lignes associée au clustering rho\n",
    "        for h in range(l): # boucle dans les colonnes associée au clustering gamma\n",
    "            Row_idx=(rho==g) \n",
    "            SliceMat=X_train[Row_idx,:].copy() # slice selon les lignes d'intérêt dans une copie de la matrice\n",
    "            Col_idx=(gamma==h)\n",
    "            SliceMat=SliceMat[:,Col_idx] # slice selon les colonnes d'intérêt \n",
    "            # calcul de mu_gh : (si w est la mesure uniforme elle se simplifie au numérateur et au dénominateur)\n",
    "            MCC[g,h]=SliceMat.mean()\n",
    "            if euclid==False: # cas de la I-divergence\n",
    "                if MCC[g,h]==0: # si mu_gh=0, on pose mu_gh=epsilon \n",
    "                    MCC[g,h]=1/(m*n)\n",
    "    return MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.64285714e+00 5.91714713e-01 4.45634921e+00 1.19846596e-06\n",
      "  3.47857143e+01]\n",
      " [6.87606112e-02 4.42263622e-02 5.68760611e-02 2.79456706e-01\n",
      "  4.32088285e-01]]\n"
     ]
    }
   ],
   "source": [
    "MCC=update_mcc(rho,gamma,X_train,k,l,euclid=False)\n",
    "print(MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Etape B : Mise à jour des clusters de ligne ($\\rho$)**\n",
    "\n",
    "Définissons une fonction qui puisse utiliser au choix la **distance euclidienne au carré** ou la **I-divergence** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_row_clusters(gamma,MCC,X_train,k,l,euclid=True):\n",
    "    m,n=np.shape(X_train)[0],np.shape(X_train)[1]\n",
    "    rho=np.zeros(m)\n",
    "    for u in range(m): # on travaille à index de ligne u fixé pour mettre à jour rho(u)\n",
    "        Valeurs_en_g=[]  # on va comparer les valeurs obtenues pour chaque g pour garder l'argmin en g \n",
    "        for g in range(k):\n",
    "            Somme_en_h=0\n",
    "            for h in range(l):\n",
    "                Zuv=X_train[u,gamma==h].copy() # slice selon la ligne u et les colonnes v t.q. gamma(v)=h\n",
    "                if euclid: # div=distance euclidienne au carré\n",
    "                    mu_gh=np.full((1,np.shape(Zuv)[1]),MCC[g,h]) # je crée une matrice ligne de meme taille que Zuv remplie de répliques de mu_gh pour pouvoir soustraire\n",
    "                    Somme_divergence_en_v=np.sum(np.power((Zuv-mu_gh),2)) \n",
    "                    \n",
    "                else: # div=I-divergence : \n",
    "                    Somme_divergence_en_v=0\n",
    "                    # Cas z_1=Zuv>0 : \n",
    "                    mu_gh=np.full((1,Zuv.count_nonzero()),MCC[g,h]) \n",
    "                    Zuv_data=np.reshape(Zuv.data,(1,Zuv.count_nonzero()))\n",
    "                    Somme_divergence_en_v+=np.sum(Zuv_data*np.log(Zuv_data/mu_gh)-(Zuv_data-mu_gh))\n",
    "                    # Cas z_1=Zuv=0 : (la divergence vaut MCC[g,h]=z_2 si Zuv=0) \n",
    "                    # le nombre d'entrées nulles de Zuv est donné par : (np.shape(Zuv)[1]-Zuv.count_nonzero())\n",
    "                    Somme_divergence_en_v+=(np.shape(Zuv)[1]-Zuv.count_nonzero())*MCC[g,h]\n",
    "                        \n",
    "                Somme_en_h+=Somme_divergence_en_v\n",
    "            Valeurs_en_g.append(Somme_en_h)\n",
    "        rho[u]=Valeurs_en_g.index(min(Valeurs_en_g)) # on retourne l'index de la valeur minimale de la liste\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temps d'exécution de notre fonction \"update_row_clusters\" pour div=euclid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une itération de la fonction update_row_clusters a pris 17.8626 secondes\n"
     ]
    }
   ],
   "source": [
    "rho,gamma=initialisation(m,n,k,l) \n",
    "tic = time.perf_counter()\n",
    "rho_next=update_row_clusters(gamma,MCC,X_train,k,l)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Une itération de la fonction update_row_clusters a pris {toc - tic:0.4f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons combien d'éléments de rho ont été modifiés après une étape de mise à jour (on constate qu'il s'agit de près de la moitié)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nb d'éléments différents entre rho et rho_next est de 608 sur un total de 1192 éléments.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Le nb d'éléments différents entre rho et rho_next est de {np.sum(rho_next!=rho)} sur un total de 1192 éléments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Etape C : Mise à jour des clusters de colonne ($\\gamma$)**\n",
    "\n",
    "Définissons une fonction qui puisse utiliser comme divergence la **distance euclidienne au carré** et la **I-divergence** au choix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_col_clusters(rho,MCC,X_train,k,l,euclid=True):\n",
    "    m,n=np.shape(X_train)[0],np.shape(X_train)[1]\n",
    "    gamma=np.zeros(n)\n",
    "    for v in range(n): # on travaille à index de colonne v fixé pour mettre à jour gamma(v)\n",
    "        Valeurs_en_h=[]  # on va comparer les valeurs obtenues pour chaque h pour garder l'argmin en h\n",
    "        for h in range(l):\n",
    "            Somme_en_g=0\n",
    "            for g in range(k):\n",
    "                Zuv=X_train[rho==g,v].copy() # slice selon la colonne v et les lignes u t.q. rho(u)=g\n",
    "                \n",
    "                if euclid: # div=distance euclidienne au carré \n",
    "                    mu_gh=np.full((np.shape(Zuv)[0],1),MCC[g,h]) # je crée une matrice colonne de meme taille que Zuv remplie de répliques de mu_gh pour pouvoir soustraire\n",
    "                    Somme_divergence_en_u=np.sum(np.power((Zuv-mu_gh),2)) \n",
    "                    \n",
    "                else: # div=I-divergence : \n",
    "                    Somme_divergence_en_u=0\n",
    "                    # Cas z_1=Zuv>0 : \n",
    "                    mu_gh=np.full((Zuv.count_nonzero(),1),MCC[g,h]) \n",
    "                    Zuv_data=np.reshape(Zuv.data,(Zuv.count_nonzero(),1))\n",
    "                    Somme_divergence_en_u+=np.sum(Zuv_data*np.log(Zuv_data/mu_gh)-(Zuv_data-mu_gh))\n",
    "                    # Cas z_1=Zuv=0 : (la divergence vaut MCC[g,h]=z_2 si Zuv=0) \n",
    "                    # le nombre d'entrées nulles de Zuv est donné par : (np.shape(Zuv)[0]-Zuv.count_nonzero())\n",
    "                    Somme_divergence_en_u+=(np.shape(Zuv)[0]-Zuv.count_nonzero())*MCC[g,h]\n",
    "                 \n",
    "                Somme_en_g+=Somme_divergence_en_u\n",
    "            Valeurs_en_h.append(Somme_en_g)\n",
    "        gamma[v]=Valeurs_en_h.index(min(Valeurs_en_h)) # on retourne l'index de la valeur minimale de la liste\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le temps d'exécution de la fonction \"update_col_clusters\" a été un point d'attention majeur de notre mise en oeuvre, c'est pourquoi nous nous assurons de son évaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mise à jour de gamma a pris 14.8750 secondes\n"
     ]
    }
   ],
   "source": [
    "tic3 = time.perf_counter()\n",
    "gamma_next=update_col_clusters(rho_next,MCC,X_train,k,l)\n",
    "toc3 = time.perf_counter()\n",
    "print(f\"La mise à jour de gamma a pris {toc3 - tic3:0.4f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette mise à jour avait un temps d'exécution de l'ordre de 8 min pour chaque itération lorsque nous manipulions une matrice de 24000 colonnes. Le gain en temps de calcul obtenu par sélection des colonnes de la matrice est donc significatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Critère de convergence par fonction objectif** \n",
    "\n",
    "Cette fonction peut, elle aussi, utiliser comme divergence la **distance euclidienne au carré** et la **I-divergence** au choix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(rho,gamma,MCC,X_train,k,l,euclid=True):\n",
    "    m,n=np.shape(X_train)[0],np.shape(X_train)[1]\n",
    "    Somme=0\n",
    "    for g in range(k):\n",
    "        for h in range(l):\n",
    "            Row_idx=(rho==g) \n",
    "            Zuv=X_train[Row_idx,:].copy() # slice selon les lignes d'intérêt dans une copie de la matrice\n",
    "            Col_idx=(gamma==h)\n",
    "            Zuv=Zuv[:,Col_idx] # slice selon les colonnes d'intérêt          \n",
    "            \n",
    "            if euclid: # avec div=euclid\n",
    "                # Créons une matrice de meme taille que Zuv remplie avec mu_gh pour faire des calculs efficaces\n",
    "                mu_gh=np.full((np.shape(Zuv)[0],np.shape(Zuv)[1]),MCC[g,h]) \n",
    "                Somme_divergence=np.sum(np.power((Zuv-mu_gh),2)) \n",
    "                \n",
    "            else: # avec div=idiv\n",
    "                Somme_divergence=0\n",
    "                # Si Zuv >0 : \n",
    "                mu_gh=np.full(np.shape(Zuv.data),MCC[g,h])\n",
    "                Somme_divergence+=np.sum(Zuv.data*np.log(Zuv.data/mu_gh)-(Zuv.data-mu_gh))\n",
    "                # Si Zuv = 0 : on ajoute z_2 x le nombre d'éléments nuls de Zuv\n",
    "                Somme_divergence+= (np.shape(Zuv)[0]*np.shape(Zuv)[1]-Zuv.count_nonzero())*MCC[g,h]\n",
    "                    \n",
    "            Somme+=Somme_divergence\n",
    "    return Somme/(m*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fonction générant l'algorithme entier**\n",
    "\n",
    "Définissons une fonction qui réunisse l'ensemble des étapes de l'algorithme. Celle-ci permet de sélectionner la divergence utilisée ainsi que le type d'initialisation (avec ou sans K-means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algo_BBAC(X_train=X_train,k=k,l=l,nb_it=100,seuil=1e-07, euclid=True, init_kmean=True): \n",
    "    m,n=np.shape(X_train)[0],np.shape(X_train)[1]\n",
    "    \n",
    "    # initialisation :\n",
    "    if init_kmean==True: # initialisation par double k-means\n",
    "        rho,gamma=init_kmeans(k,l,X_train)\n",
    "    else: # initialisation aléatoire\n",
    "        rho,gamma=initialisation(m,n,k,l)\n",
    "        \n",
    "    Obj_list =[math.inf] # liste des valeurs de la fonction objectif \n",
    "    it=0 # compteur d'itérations \n",
    "    critere=True # pour rentrer dans la boucle while \n",
    "    while(critere==True and it<nb_it):\n",
    "        it=it+1\n",
    "        MCC=update_mcc(rho,gamma,X_train,k,l,euclid) # Etape A\n",
    "        rho=update_row_clusters(gamma,MCC,X_train,k,l,euclid) # Etape B\n",
    "        gamma=update_col_clusters(rho,MCC,X_train,k,l,euclid) # Etape C\n",
    "        # calcul du critère de convergence via la fonction objectif\n",
    "        Obj = objective_function(rho,gamma,MCC,X_train,k,l,euclid) # calcul de fonction objectif\n",
    "        #print(Obj) # optionnel : on imprime à chaque itération pour suivre l'avancée de l'algorithme \n",
    "        Obj_list.append(Obj)\n",
    "        if ((np.abs(Obj_list[-1]-Obj_list[-2]))<seuil): # calcul de la différence relative\n",
    "            critere=False \n",
    "            \n",
    "    Obj_list=Obj_list[1:] # on retire la première valeur infinie de la liste    \n",
    "    return rho, gamma, Obj_list, it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Analyse des résultats (pour $l=5$)\n",
    "\n",
    "#### Lançons nos algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Ne pas exécuter cette section \n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lançons l'algorithme pour la distance euclidienne avec initialisation K-means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_euc_k = time.perf_counter()\n",
    "rho_euc_k,gamma_euc_k,Obj_list_euc_k,it_euc_k=Algo_BBAC()\n",
    "toc_euc_k = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'algorithme a pris 266.9476 minutes\n",
      "Nombre d'itérations : 6\n",
      "Valeur initiale de la fonction objectif : 0.23841419658042906\n",
      "Valeur finale de la fonction objectif : 0.23693556528522636\n"
     ]
    }
   ],
   "source": [
    "print(f\"L'algorithme a pris {toc_euc_k - tic_euc_k :0.4f} minutes\")\n",
    "print('Nombre d\\'itérations :', it_euc_k)\n",
    "print('Valeur initiale de la fonction objectif :',Obj_list_euc_k[0])\n",
    "print('Valeur finale de la fonction objectif :',Obj_list_euc_k[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lançons-le pour la distance euclidienne avec initialisation aléatoire uniforme (sans K-means) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_euc = time.perf_counter()\n",
    "rho_euc,gamma_euc,Obj_list_euc,it_euc=Algo_BBAC(init_kmean=False) \n",
    "toc_euc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'algorithme a pris 1213.7760 secondes\n",
      "Nombre d'itérations : 34\n",
      "Valeur initiale de la fonction objectif : 0.2767017248569705\n",
      "Valeur finale de la fonction objectif : 0.23711828823667958\n"
     ]
    }
   ],
   "source": [
    "print(f\"L'algorithme a pris {toc_euc - tic_euc:0.4f} secondes\")\n",
    "print('Nombre d\\'itérations :', it_euc)\n",
    "print('Valeur initiale de la fonction objectif :',Obj_list_euc[0])\n",
    "print('Valeur finale de la fonction objectif :',Obj_list_euc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lançons l'algorithme pour la I-divergence avec initialisation K-means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_idiv_k = time.perf_counter()\n",
    "rho_idiv_k,gamma_idiv_k,Obj_list_idiv_k,it_idiv_k=Algo_BBAC(euclid=False)\n",
    "toc_idiv_k = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'algorithme a pris 934.5799 secondes\n",
      "Nombre d'itérations : 33\n",
      "Valeur initiale de la fonction objectif : 0.18474644558737605\n",
      "Valeur finale de la fonction objectif : 0.16713686475341455\n"
     ]
    }
   ],
   "source": [
    "print(f\"L'algorithme a pris {toc_idiv_k - tic_idiv_k:0.4f} secondes\")\n",
    "print('Nombre d\\'itérations :', it_idiv_k)\n",
    "print('Valeur initiale de la fonction objectif :',Obj_list_idiv_k[0])\n",
    "print('Valeur finale de la fonction objectif :',Obj_list_idiv_k[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, lançons l'algorithme pour la I-divergence avec initialisation aléatoire uniforme (sans K-means) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_idiv = time.perf_counter()\n",
    "rho_idiv,gamma_idiv,Obj_list_idiv,it_idiv=Algo_BBAC(euclid=False,init_kmean=False)\n",
    "toc_idiv = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'algorithme a pris 555.9876 secondes\n",
      "Nombre d'itérations : 20\n",
      "Valeur initiale de la fonction objectif : 0.20662185474697412\n",
      "Valeur finale de la fonction objectif : 0.16932606658484853\n"
     ]
    }
   ],
   "source": [
    "print(f\"L'algorithme a pris {toc_idiv - tic_idiv:0.4f} secondes\")\n",
    "print('Nombre d\\'itérations :', it_idiv)\n",
    "print('Valeur initiale de la fonction objectif :',Obj_list_idiv[0])\n",
    "print('Valeur finale de la fonction objectif :',Obj_list_idiv[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction des résultats obtenus pour la distance euclidienne avec initialisation K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rho_euc_k.txt\", \"w\") as external_file:\n",
    "    for index in rho_euc_k:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gamma_euc_k.txt\", \"w\") as external_file:\n",
    "    for index in gamma_euc_k:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Obj_list_euc_k.txt\", \"w\") as external_file:\n",
    "    for index in Obj_list_euc_k:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction des résultats obtenus pour la I-divergence avec initialisation K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rho_idiv_k.txt\", \"w\") as external_file:\n",
    "    for index in rho_idiv_k:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gamma_idiv_k.txt\", \"w\") as external_file:\n",
    "    for index in gamma_idiv_k:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Obj_list_idiv_k.txt\", \"w\") as external_file:\n",
    "    for index in Obj_list_idiv_k:\n",
    "        add_text = index\n",
    "        print(add_text, file=external_file)\n",
    "    external_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Fin de la section à ne pas exécuter\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a reproduit ci-dessous les valeurs de rho et gamma obtenues (pour nos deux divergences avec initialisation K-means) afin de faciliter notre analyse des résultats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_euc_k=np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
    "gamma_euc_k=np.array([4.0,2.0,4.0,4.0,4.0,3.0,4.0,4.0,4.0,3.0,4.0,4.0,0.0,0.0,1.0,4.0,0.0,4.0,0.0,4.0,3.0,3.0,3.0,1.0,0.0,0.0,3.0,0.0,0.0,1.0,3.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,3.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,3.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
    "Obj_list_euc_k=np.array([0.23841419658042906,0.2371260830510876,0.23698907446141834,0.236942949374856,0.23693556528522636,0.23693556528522636])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_idiv_k=np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0])\n",
    "gamma_idiv_k=np.array([4.0,4.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,3.0,3.0,2.0,3.0,3.0,2.0,3.0,2.0,2.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,1.0,3.0,1.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,3.0,1.0,3.0,3.0,1.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,3.0,1.0,3.0,1.0,3.0,3.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
    "Obj_list_idiv_k=np.array([0.18474644558737605,0.17725568073833234,0.17368773733588488,0.17127691476156223,0.16993398887421762,0.16907696036292547,0.16867489175156752,0.16844201963777244,0.16826467568399806,0.16817674091301676,0.1681340844592741,0.16811497866610045,0.16806211662325615,0.16803390114730443,0.1678988483222972,0.1678672820110701,0.16783820119512796,0.1678214480690782,0.16779668126089622,0.1676463913627704,0.16742528691380235,0.16732402379195316,0.16729219277394333,0.16727858267080659,0.1672506899182153,0.16723475690103315,0.16722302758615198,0.16721143368968838,0.16718766425094844,0.16713895000338794,0.1671371175249012,0.16713692045093162,0.16713686475341455])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1) Comparaison des résultats pour différentes initialisations et divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour nos deux divergences, on constate (à permutation près) que la fonction assignation $\\rho$ obtenue avec ou sans initialisation K-means donne, à un élément près sur un total de 1192, les mêmes résultats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour la distance euclidienne avec et sans initialisation K-means:\n",
      "Nombre de lignes assignées au même label : 1191 sur 1192\n",
      "\n",
      "Résultats pour la I-divergence avec et sans initialisation K-means:\n",
      "Nombre de lignes assignées au même label : 1191 sur 1192\n"
     ]
    }
   ],
   "source": [
    "print(\"Résultats pour la distance euclidienne avec et sans initialisation K-means:\")\n",
    "print(f\"Nombre de lignes assignées au même label : {np.sum(rho_euc!=rho_euc_k)} sur 1192\")\n",
    "print(\"\\nRésultats pour la I-divergence avec et sans initialisation K-means:\")\n",
    "print(f\"Nombre de lignes assignées au même label : {np.sum(rho_idiv==rho_idiv_k)} sur 1192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de nos premières tentatives avec $l=2$ et sans étape de prétraitement (nous gardions seulement les 500 mots les plus fréquents), l'algorithme BBAC avec initialisation K-means ne modifiait pas du tout les valeurs de $\\rho$ et $\\gamma$ car le critère de convergence était immédiatement atteint grâce à l'initialisation. Ce n'est plus le cas depuis la mise en place d'un prétraitement plus lourd et le recours à $l=5$ clusters colonnes. On observe un réel comportement itératif. \n",
    "\n",
    "Comme attendu :\n",
    "* on peut vérifier que la fonction objectif diminue bien à chaque étape ;\n",
    "* on obtient qu'à divergence fixée, les algorithmes convergent vers une fonction objectif finale d'un ordre de grandeur comparable (bien que la valeur initiale de la fonction objectif ait pu être différente dans le cas d'initialisation différente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2) Degré de précision des labels estimés\n",
    "\n",
    "On analysera les résultats obtenus pour des initialisations par K-means seulement (puisque les résultats sont comparables à ceux obtenus sans cette initialisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats après convergence pour la distance euclidienne au carré:\n",
      "On a 14 lignes de cluster ligne 0 et 1178 lignes de cluster ligne 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Résultats après convergence pour la distance euclidienne au carré:\")\n",
    "print(f\"On a {sum(rho_euc_k==0)} lignes de cluster ligne 0 et {sum(rho_euc_k==1)} lignes de cluster ligne 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats après convergence pour la I-divergence:\n",
      "On a 170 lignes de cluster ligne 0 et 1022 lignes de cluster ligne 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRésultats après convergence pour la I-divergence:\")\n",
    "print(f\"On a {sum(rho_idiv_k==0)} lignes de cluster ligne 0 et {sum(rho_idiv_k==1)} lignes de cluster ligne 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Matrices de confusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion pour div=euclid : \n",
      " [[ 14 579]\n",
      " [  0 599]]\n"
     ]
    }
   ],
   "source": [
    "Confusion_euc=contingency_matrix(newsgroups_train.target, rho_euc_k)\n",
    "print(\"Matrice de confusion pour div=euclid : \\n\", Confusion_euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion pour div=idiv : \n",
      " [[ 65 528]\n",
      " [105 494]]\n"
     ]
    }
   ],
   "source": [
    "Confusion_idiv=contingency_matrix(newsgroups_train.target, rho_idiv_k)\n",
    "print(\"Matrice de confusion pour div=idiv : \\n\", Confusion_idiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Information mutuelle normalisée (NMI)**$^{[8]}$\n",
    "\n",
    "Voyons les résultats d'information mutuelle entre les vecteurs de labels réels et ceux de labels estimés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI obtenue pour la distance euclidienne : 0.02185116388868639\n",
      "NMI obtenue pour la I-divergence : 0.008064821484925518\n"
     ]
    }
   ],
   "source": [
    "print(\"NMI obtenue pour la distance euclidienne :\",normalized_mutual_info_score(newsgroups_train.target, rho_euc_k))\n",
    "print(\"NMI obtenue pour la I-divergence :\",normalized_mutual_info_score(newsgroups_train.target, rho_idiv_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La très faible valeur des NMI obtenue coincide avec le fait que les matrices de confusion ne sont pas du tout diagonales à permutation près. \n",
    "\n",
    "Il semble que :\n",
    "\n",
    "* la distance euclidienne paye le fait qu'elle ait assigné presque toutes les lignes (1180 sur 1192) au même label. En revanche, les 12 lignes mises à part l'ont été à raison ; \n",
    "* la I-divergence paye le fait d'avoir assigné une majeure partie des lignes (1022 sur 1192) au même label ainsi que le fait qu'elle ait mal identifié les 170 lignes qu'elle a mises à part puisque 65 d'entre elles auraient dû rester assignées au label \"majoritaire\" (celui qui a aspiré la majeure partie des lignes).\n",
    "\n",
    "L'analyse des matrices de confusion et de la NMI montre donc que la I-divergence présente de moins bons résultats que la distance euclidienne dans le cadre de notre application. Sur ce point, nos résultats sont donc différents de ceux des auteurs de l'article BBAC$^{[1]}$ qui, lors du clustering de la base textuelle CLASSIC3, ont trouvé de meilleurs résultats pour la I-divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Comparaison des résultats obtenus par clustering unidimensionnel (K-means)**\n",
    "\n",
    "Un des objectifs de l'article BBAC$^{[1]}$ était de montrer que le co-clustering permettait d'avoir de meilleurs résultats que le clustering simple ou unidimensionnel, comme K-means. De plus, ses auteurs mentionnent dans leur application à l'analyse textuelle (p. 1956) utiliser un algorithme de clustering unidimensionnel (le \"spherical k-means\") comme \"étalon\" afin d'identifier le possible gain obtenu par leur procédure de double clustering simultané.\n",
    "\n",
    "Nous utiliserons pour notre part le K-means classique comme \"étalon\" afin d'observer si l'approche de co-clustering a permis de raffiner nos résultats par rapport à un clustering simple. Appliquons l'algorithme K-means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_single_clust,gamma_single_clust=init_kmeans(k,l,X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion pour k-means : \n",
      " [[579  14]\n",
      " [599   0]]\n"
     ]
    }
   ],
   "source": [
    "Confusion_kmeans=contingency_matrix(newsgroups_train.target, rho_single_clust)\n",
    "print(\"Matrice de confusion pour k-means : \\n\", Confusion_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI obtenue pour K-means : 0.02185116388868639\n"
     ]
    }
   ],
   "source": [
    "print(\"NMI obtenue pour K-means :\",normalized_mutual_info_score(newsgroups_train.target, rho_single_clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison du co-clustering avec distance euclidienne au carré et du clustering avec k-means\n",
      "Nombre de posts classifiés de la même façon sur un total de 1192 : 1192\n",
      "\n",
      "Comparaison du co-clustering avec I-divergence et du clustering avec k-means\n",
      "Nombre de posts classifiés de la même façon sur un total de 1192 : 1036\n"
     ]
    }
   ],
   "source": [
    "print('Comparaison du co-clustering avec distance euclidienne au carré et du clustering avec k-means')\n",
    "print('Nombre de posts classifiés de la même façon sur un total de 1192 :',np.sum(rho_euc_k!=rho_single_clust))\n",
    "\n",
    "print('\\nComparaison du co-clustering avec I-divergence et du clustering avec k-means')\n",
    "print('Nombre de posts classifiés de la même façon sur un total de 1192 :',np.sum(rho_idiv_k!=rho_single_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que la NMI obtenue pour K-means est également très faible et est égale à celle obtenue par BBAC avec distance euclidienne. Si l'on observe aussi les matrices de confusion, et le nombre de posts labellés de la même façon, il apparaît que l'algorithme BBAC, en partant de l'initialisation K-means (avec distance euclidienne) a procédé à des itérations (puisqu'il n'a pas immédiatement convergé) et donc à des changements de label sur rho et / ou gamma mais a tout de même fini avec le même partitionnement des lignes que K-means. La I-divergence, pour sa part, semble avoir dégradé au cours de ses itérations la classification initiale effectuée par K-means sur les lignes de la matrice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Choix de $l$, le nombre de clusters colonne\n",
    "\n",
    "Il n'est pas certain que choisir $l=5$ soit la seule - ni même la meilleure - option dans notre contexte. L'algorithme donné par l'article BBAC$^{[1]}$ n'explore pas cette question et se concentre sur l'aspect algorithmique une fois les valeurs $k$ et $l$ données. Il est pourtant possible que l'algorithme sépare plus justement les posts en se basant sur plus ou moins de clusters colonnes. Traiter cette question est donc important lorsqu'il s'agit de faire un choix informé de modèle.\n",
    "\n",
    "Si l'article ne répond pas à cette problématique, des outils statistiques existent dans ce sens. Ainsi, afin d'investiguer l'impact du choix initial de la variable $l$ nous allons comparer les résultats obtenus - à $k=2$ fixé - pour un nombre de clusters colonnes qui varie de 5 à 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian information criterion (BIC)\n",
    "\n",
    "Un outil classique permettant de comparer des résultats de logvraisemblance obtenus par différents modèles est le *Bayesian information criterion* (BIC). Celui-ci contrôle l'impact du nombre de paramètres des modèles en ajoutant une pénalité au calcul de logvraisemblance. Ce critère indique de sélectionner le modèle associé à la plus petite valeur BIC calculée selon la formule suivante :\n",
    "\n",
    "Critère BIC : $\\frac{P}{2} \\log(T)-\\log(\\hat{L})$ où :\n",
    "\n",
    "* $T$ est la taille de l'échantillon : ici $T = nm$\n",
    "\n",
    "* $P$ est le nombre de paramètres : ici $P = kl-1$ (cela dépend des contraintes du modèle)\n",
    "\n",
    "* Comme mentionné en introduction, nous n'avons pas de cadre probabiliste permettant d'avoir un accès direct à la logvraisemblance. Néanmoins, lorsque $d_{\\phi}$ est la distance euclidienne au carré, celle-ci peut s'interpréter comme la logvraisemblance dans un modèle gaussien car on a l'égalité $-\\log(\\hat{L}) = (nm) \\mathbb{E}[d_{\\phi}(z_{uv},\\widehat{\\mu_{\\rho(u)\\gamma(v)}})]+c\\text{, avec }c\\in \\mathbb{R} \\text{ une constante}$ (et les argmin coïncident)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En effet, on a :\n",
    "\n",
    "* d'une part, notre fonction objectif qui donne (avec $ u \\in \\{1,...,m\\}, v\\in\\{1,...,n\\}$):\n",
    "\n",
    "$\\mathbb{E}[d_{\\phi}(z_{uv},\\mu_{\\rho(u)\\gamma(v)})] =\\frac{1}{nm}\\sum_{u,v}(z_{uv}-\\mu_{\\rho(u)\\gamma(v)})^{2}$\n",
    "\n",
    "* d'autre part, si on considère $z_{uv} \\sim_{iid} \\mathcal{N}(\\mu_{\\rho(u)\\gamma(v)},\\frac{1}{2})\\text{ }\\forall u,v$ et $f$ la densité associée:\n",
    "\n",
    "$\\prod_{u,v} f(z_{uv})=\\prod_{u,v} \\frac{1}{\\sqrt{\\pi}}exp(-(z_{uv}-\\mu_{\\rho(u)\\gamma(v)})^{2})$\n",
    "\n",
    "$-\\log (\\prod_{u,v}f(z_{uv})) = \\sum_{u,v} [ (z_{uv}-\\mu_{\\rho(u)\\gamma(v)})^{2} + \\log(\\sqrt{\\pi}) ]$ \n",
    "\n",
    "d'où :\n",
    "\n",
    "$-\\log(\\hat{L}) = (nm) \\mathbb{E}[d_{\\phi}(z_{uv},\\widehat{\\mu_{\\rho(u)\\gamma(v)}})]+c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons donc une fonction permettant d'évaluer le critère BIC. Celui-ci n'aura de sens que dans le cas où la distance euclidienne est utilisée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=n*m # taille de l'échantillon\n",
    "P=k*l-1 # nombre de paramètres\n",
    "obj=Obj_list_euc_k[-1] # dernière valeur de la fonction objectif\n",
    "\n",
    "def BIC_crit(obj,T,P):\n",
    "    return (P/2)*np.log(T)+obj*T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisons l'information mutuelle normalisée (NMI) et le critère BIC en fonction du nombre de clusters de colonnes $l$\n",
    "\n",
    "Nous allons nous inspirer de l'analyse menée par les auteurs de l'article BBAC$^{[1]}$ qui ont comparé la performance de leurs modèles en fonction du nombre de clusters colonne considéré à l'aide de la NMI (p. 1957). Travailler avec l'information mutuelle entre le vecteur de labels estimés et le vecteur de labels réels est une alternative à l'analyse BIC détaillée plus haut. Les calculs de NMI ne reposant sur aucun modèle probabiliste, il est possible de les effectuer pour la I-divergence comme pour la distance euclidienne. Au vu des temps de calcul nécessaires, nous nous limiterons néanmoins à l'évaluation des valeurs de NMI pour la distance euclidienne et ce, afin de pouvoir comparer ces résultats avec ceux obtenus par critère BIC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Ne pas exécuter cette section du code\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calcul de la NMI et du critère BIC pour la distance euclidienne avec $5 \\leq l \\leq 15$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "NMI_euc_vec=np.zeros(11)\n",
    "BIC_euc_vec=np.zeros(11)\n",
    "# on connait déjà les résultats pour l=5 :\n",
    "NMI_euc_vec[0]=normalized_mutual_info_score(newsgroups_train.target, rho_euc_k)\n",
    "BIC_euc_vec[0]=BIC_crit(obj=Obj_list_euc_k[-1],T=T,P=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=1\n",
    "for l_var in range(6,16): \n",
    "    rho_euc_1,gamma_euc_1,Obj_list_euc_1,it_euc_1=Algo_BBAC(l=l_var)\n",
    "    NMI_euc_vec[c]=normalized_mutual_info_score(newsgroups_train.target, rho_euc_1)\n",
    "    P=k*l_var-1\n",
    "    BIC_euc_vec[c]=BIC_crit(obj=Obj_list_euc_1[-1],T=T,P=P)\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Fin de la section à ne pas exécuter\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a extrait et reproduit ci-dessous les résultats obtenus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMI_euc_vec=np.array([0.02185116, 0.02037882, 0.01889534, 0.02037882, 0.02185116, 0.02185116, 0.02185116, 0.02037882, 0.02185116, 0.02185116, 0.02185116])\n",
    "BIC_euc_vec=np.array([197760.39078081, 196303.76415294, 196049.19311119, 195284.16569096,194820.15789061, 194620.50651424, 194486.18252316, 194453.82973799, 194285.74730155, 194247.21301834, 194104.3091315])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "abs=np.arange(5,16,1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"NMI de BBAC pour 20 Newsgroup (avec k=2)\")\n",
    "plt.plot(abs,NMI_euc_vec) \n",
    "plt.ylim((0,0.06))\n",
    "plt.xlabel(\"Nombre de clusters colonnes ($5\\leq l \\leq 15)$\") ; plt.ylabel(\"NMI\") ;\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces résultats sont en accord avec ceux de l'article BBAC$^{[1]}$ qui, pour la base CLASSIC3, obtenaient peu de fluctuation dans les valeurs de NMI obtenues malgré une variation dans le nombre de clusters colonne $l$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"Critère BIC pour 20 Newsgroup (avec k=2)\")\n",
    "plt.plot(abs,BIC_euc_vec,c=\"red\") \n",
    "plt.xlabel(\"Nombre de clusters colonnes ($5\\leq l \\leq 15)$\") ; plt.ylabel(\"BIC\") ;\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats BIC orientent le choix du nombre optimal de clusters colonne, qui est ici donné par la valeur de l'argmin $l=15$. Cette valeur est donc censée traduire le meilleur compromis entre la maximisation de la logvraisemblance et la complexité du modèle parmi les valeurs $l \\in\\{5,...,15\\}$. Il semble donc que pour ces valeurs de $l$, la pénalisation croissante en fonction du nombre de paramètres ne suffise pas à contrebalancer les bénéfices obtenus sur la vraisemblance lorsque l'on augmente la valeur de $l$.\n",
    "\n",
    "De plus, le fait que la courbe soit décroissante sur tout l'intervalle considéré suggère qu'il serait intéressant de calculer les résultats BIC pour des valeurs de $l$ plus grandes encore dans l'espoir de trouver un point d'inflexion de la courbe qui indiquerait un minimum, au moins local, du critère (et non pas un minimum au bord comme c'est le cas ici). Nos capacités de calcul étant limitées, nous avons dû nous limiter aux valeurs présentées ici, mais il serait souhaitable d'effectuer des calculs pour $l$ variant de 5 à 50 par exemple, comme l'ont fait les auteurs de l'article BBAC.\n",
    "\n",
    "Puisque la valeur du critère BIC nous a désigné $l=15$ comme nombre de clusters optimal, nous avons extrait les résultats de $\\rho$ et $\\gamma$ pour cette valeur de $l$ afin de faire quelques commentaires de conclusion. De par le graphique de NMI, il est clair que faire le choix de $l=15$ ne permettra pas d'obtenir des niveaux bien meilleurs sur cette métrique. Observons donc plutôt la matrice de confusion obtenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résultats obtenus pour l=15\n",
    "rho_l_15=np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
    "gamma_l_15=np.array([2.0,9.0,8.0,8.0,8.0,14.0,8.0,8.0,8.0,4.0,8.0,8.0,12.0,12.0,13.0,8.0,12.0,13.0,12.0,8.0,5.0,5.0,5.0,11.0,12.0,12.0,5.0,12.0,12.0,1.0,5.0,12.0,12.0,12.0,1.0,13.0,12.0,12.0,12.0,0.0,12.0,12.0,12.0,12.0,12.0,0.0,0.0,0.0,12.0,12.0,12.0,0.0,12.0,0.0,12.0,12.0,0.0,12.0,12.0,6.0,0.0,1.0,12.0,12.0,12.0,10.0,5.0,12.0,12.0,11.0,12.0,6.0,11.0,12.0,6.0,12.0,12.0,3.0,7.0,0.0,3.0,12.0,12.0,12.0,12.0,12.0,3.0,12.0,12.0,7.0,11.0,1.0,3.0,3.0,0.0,7.0,0.0,1.0,3.0,10.0,12.0,3.0,11.0,12.0,6.0,0.0,12.0,0.0,12.0,11.0,12.0,10.0,10.0,6.0,7.0,12.0,0.0,0.0,11.0,7.0,1.0,11.0,6.0,10.0,7.0,7.0,3.0,12.0,7.0,10.0,7.0,10.0,7.0,3.0,3.0,10.0,6.0,12.0,7.0,10.0,11.0,7.0,7.0,10.0,11.0,12.0,1.0,7.0,12.0,7.0,7.0,6.0,7.0,11.0,10.0,7.0,10.0,7.0,10.0,7.0,10.0,10.0,12.0,6.0,7.0,7.0,11.0,11.0,11.0,7.0,10.0,12.0,10.0,11.0,10.0,7.0,11.0,10.0,7.0,6.0,7.0,6.0,7.0,7.0,10.0,3.0,10.0,7.0,7.0,7.0,7.0,7.0,10.0,7.0,10.0,10.0,11.0,10.0,7.0,1.0,10.0,7.0,7.0,10.0,6.0,10.0,10.0,10.0,6.0,10.0,11.0,0.0,10.0,7.0,6.0,6.0,10.0,7.0,1.0,6.0,3.0,7.0,7.0,7.0,10.0,3.0,6.0,7.0,7.0,7.0,6.0,7.0,7.0,10.0,7.0,10.0,11.0,10.0,6.0,6.0,6.0,7.0,6.0,10.0,7.0,10.0,10.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,7.0,3.0,7.0,7.0,7.0,7.0,7.0,10.0,3.0,10.0,10.0,7.0,7.0,7.0,6.0,10.0,3.0,7.0,6.0,7.0,6.0,7.0,7.0,10.0,6.0,11.0,7.0,10.0,11.0,6.0,7.0,7.0,10.0,7.0,6.0,6.0,6.0,10.0,3.0,7.0,6.0,7.0,10.0,7.0,7.0,3.0,7.0,7.0,10.0,7.0,10.0,7.0,7.0,6.0,10.0,6.0,7.0,6.0,10.0,10.0,3.0,7.0,3.0,7.0,7.0,3.0,7.0,7.0,7.0,10.0,7.0,10.0,10.0,7.0,7.0,3.0,6.0,11.0,10.0,7.0,10.0,10.0,10.0,10.0,7.0,7.0,10.0,7.0,7.0,7.0,10.0,10.0,10.0,10.0,10.0,6.0,3.0,10.0,7.0,7.0,7.0,7.0,7.0,10.0,3.0,7.0,7.0,6.0,10.0,7.0,6.0,6.0,7.0,7.0,7.0,11.0,7.0,7.0,6.0,7.0,6.0,3.0,10.0,7.0,7.0,3.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,10.0,7.0,3.0,7.0,3.0,7.0,10.0,10.0,7.0,7.0,7.0,7.0,6.0,7.0,10.0,10.0,7.0,7.0,7.0,7.0,7.0,10.0,7.0,6.0,7.0,10.0,7.0,6.0,6.0,7.0,3.0,10.0,7.0,7.0,7.0,7.0,10.0,7.0,7.0,3.0,7.0,7.0,10.0,7.0,6.0,10.0,6.0,7.0,7.0,6.0,7.0,7.0,7.0,7.0,6.0,7.0,6.0,6.0,7.0,7.0,7.0,7.0,3.0,7.0,7.0,10.0,10.0,7.0,10.0,7.0,7.0,3.0,7.0,10.0,6.0,7.0,7.0,7.0,6.0,10.0,6.0,7.0,7.0,7.0,7.0,7.0,7.0,10.0,7.0,7.0,10.0,10.0,7.0,7.0,6.0,7.0,7.0,7.0,10.0,7.0,7.0,3.0,10.0,7.0,7.0,7.0,10.0,10.0,6.0,6.0,7.0,10.0,7.0,10.0,7.0,6.0,7.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,10.0,10.0,6.0,10.0,7.0,7.0,3.0,7.0,7.0,7.0,10.0,7.0,3.0,6.0,7.0,7.0,7.0,7.0,3.0,7.0,7.0,10.0,10.0,7.0,7.0,7.0,10.0,7.0,6.0,7.0,10.0,7.0,10.0,10.0,7.0,10.0,7.0,10.0,7.0,7.0,7.0,7.0,3.0,7.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,6.0,7.0,10.0,7.0,7.0,10.0,10.0,7.0,10.0,7.0,10.0,7.0,10.0,7.0,7.0,7.0,6.0,7.0,10.0,7.0,7.0,10.0,10.0,6.0,10.0,7.0,10.0,6.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,10.0,10.0,10.0,7.0,10.0,7.0,6.0,7.0,3.0,10.0,7.0,7.0,7.0,10.0,10.0,10.0,7.0,7.0,10.0,3.0,7.0,7.0,7.0,6.0,7.0,7.0,7.0,6.0,10.0,7.0,7.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,6.0,7.0,6.0,7.0,7.0,10.0,7.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,6.0,6.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0,10.0,7.0,7.0,10.0,10.0,10.0,7.0,7.0,7.0,10.0,7.0,6.0,6.0,7.0,6.0,10.0,10.0,7.0,7.0,7.0,10.0,7.0,7.0,7.0,7.0,7.0,10.0,10.0,7.0,10.0,10.0,7.0,6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion pour l=15 (et la distance euclidienne)\n",
      "[[ 14 579]\n",
      " [  0 599]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrice de confusion pour l=15 (et la distance euclidienne)\")\n",
    "Confusion_l_15=contingency_matrix(newsgroups_train.target, rho_l_15)\n",
    "print(Confusion_l_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous reconnaissons la matrice de confusion que nous avions déjà obtenue pour BBAC avec $l=5$ et la distance euclidienne mais aussi lors de nos calculs \"étalon\" pour un simple K-means. De nouveau, il semble que la minimisation de la fonction objectif n'ait pas d'impact sur la qualité du partitionnement en ligne de la matrice. En effet, bien que nous ayons choisi le modèle qui minimise le critère BIC, l'impact ne se perçoit pas sur les valeurs de $\\rho$. Notre fonction objectif ne semble pas destinée à la recherche de clusters ligne optimaux pour la matrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dernière démarche \"étalon\", calculons la valeur de la dite fonction objectif pour la vraie valeur de $\\rho$ (le vecteur des labels réels, contenu dans notre variable **newsgroups_train.target**). Notons qu'il nous faut également une valeur pour $\\gamma$ ce que les données d'origine ne possèdent pas. Nous utilisons donc la valeur **gamma_l_15**, optimale au sens du critère BIC, que nous avons obtenue pour un modèle ayant $l=15$ clusters colonnes. Afin d'avoir des résultats comparables à ceux de cette section, nous choisirons la version de la fonction objectif associée à la distance euclidienne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeur de la fonction objectif obtenue de manière artificielle sur les vrais labels mais sans apprentissage:\n",
      " 0.2699820733829422\n",
      "\n",
      "Valeur finale de la fonction objectif obtenue pour l=15 avec div=euclid et initialisation par K-means:\n",
      " 0.23239047140802377\n"
     ]
    }
   ],
   "source": [
    "# il nous faut définir MCC pour effectuer le calcul de fonction objectif\n",
    "MCC=update_mcc(newsgroups_train.target,gamma_l_15,X_train,k=2,l=15,euclid=True)\n",
    "# calcul de la fonction objectif \n",
    "obj_f=objective_function(newsgroups_train.target,gamma_l_15,MCC,X_train,k=2,l=15,euclid=True)\n",
    "print(\"Valeur de la fonction objectif obtenue de manière artificielle sur les vrais labels mais sans apprentissage:\\n\",obj_f)\n",
    "print(\"\\nValeur finale de la fonction objectif obtenue pour l=15 avec div=euclid et initialisation par K-means:\\n\",Obj_list_euc_1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérifions que la matrice de confusion des vrais labels est bien diagonale : \n",
      "[[593   0]\n",
      " [  0 599]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vérifions que la matrice de confusion des vrais labels est bien diagonale : \")\n",
    "print(contingency_matrix(newsgroups_train.target, newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate donc qu'un vecteur $\\rho$ ayant la meilleure matrice de confusion possible (parfaitement diagonale) a pourtant une valeur de fonction objectif plus élevée que certains de nos estimateurs. De nouveau, ce résultat semble montrer que la minimisation de la fonction objectif et le partitionnement idéal des lignes de la matrice ne coïncident pas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
